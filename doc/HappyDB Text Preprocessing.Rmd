---
title: "HappyDB EDA self study code"
output: html_notebook
---


```{r load libraries, message=FALSE, warning=FALSE}
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
```

```{r load data, message=FALSE, warning=FALSE}
cleaned_hm <- read_csv(
  file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243 Liu/Project-1/data/cleaned_hm.csv')
# the read_csv is faster comparing to read.csv, especially for large file
# original_hm <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/original_hm.csv')
# demographic <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/demographic.csv')
# senselabel <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/senselabel.csv')
# vad <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/vad.csv')
```

```{r rewrite code without pipeline, warning=FALSE, message=FALSE}
text <- cleaned_hm$cleaned_hm
length(grep("[A-Z]{5,}", text)) # check if there exists five consecutive capital letters
#length is nonzero, so there are moments with consecutive five capital letter (in another word, have not been lowercased)
#method one
# lowertext <- tolower(text)
#method 2, use tidytext
# need to create a corpus first
text_corpus <- VCorpus(VectorSource(text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
# the content_transformer simply creates a function, which is tolower in deault fun, and apply to text through tm_map
# with the content transformer, you can create specified fcn for further use
text_corpus <- tm_map(text_corpus, removePunctuation)
text_corpus <- tm_map(text_corpus, removeNumbers)
text_corpus <- tm_map(text_corpus, removeWords, words = character(0))
# in the tm_map fcn, tm_map(text, fcn, fcn arguments)
text_corpus <- tm_map(text_corpus, stripWhitespace)
# stripWhitespace will turn multiple spaces into one space blank

#stem words and transfer to tidy tibble
stemmed_text <- tm_map(text_corpus, stemDocument)
stemmed_text <- tidy(stemmed_text)
stemmed_text <- select(stemmed_text, text)

#create dictionary and tokenizing
dict <- tidy(text_corpus)
dict <- select(dict, text)
dict <- unnest_tokens(dict, dictionary, text)
# unnest_tokens(data, output column name, input column name)
# unnest_token basically split words in sentences


# Adding custom stopwords (i.e meaningless words that wont help in further analysis)
word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past")
# the name here has to be the same as the stop_words column name, which is 'word'
# so that it can be binded successfully
stop_dict <- bind_rows(stop_words, data.frame(word))
stop_dict[!(stop_dict$lexicon %in% unique(stop_dict$lexicon)[-4]), 'lexicon'] <- 'updated'

# combine stemmed words and dictionary together
completed_text <- mutate(stemmed_text, id = row_number())
# mutate adds another variable, which is row_number, so that when you unnest sentences into words
# you will know which word in which sentence by the id variable(row_number)
completed_text <- unnest_tokens(completed_text, stems, text)
completed_text <- bind_cols(completed_text, dict)
completed_text <- anti_join(completed_text, stop_dict, by = c('dictionary' = 'word'))
# the anti_join will return rows in the first table that have no matches on the second table
# by = c('dict' = 'word') works when same variable in two tabels have different names
# so it can treat these two variabels as the same one , and join by this variable

#get frequency using group_by and count
copy <- completed_text
completed_text <- group_by(completed_text, stems)
completed_text <- count(completed_text, dictionary)
completed_text <- mutate(completed_text, word = dictionary[which.max(n)])
# the group_by fcn affects here, so wordd variable will show the most frequent word given they have the same stems
completed_text <- ungroup(completed_text)
completed_text <- select(completed_text, stems, word)
completed_text <- distinct(completed_text)
#here it doesnt specify which variabe to check uniqueness so use all of them
completed_text <- right_join(completed_text, copy)
completed_text <- select(completed_text, -stems)
                        

# put words abck to sentences according to the id
completed_text <- group_by(completed_text, id)
completed_text <- summarise(completed_text, text = str_c(word, collapse = ' '))
completed_text <- ungroup(completed_text)

#put it back to the original dataframe
revised_df <- cleaned_hm
revised_df <- mutate(revised_df, id = row_number())
revised_df <- inner_join(revised_df, completed_text)

#datatable(revised_df)
write.csv(revised_df, "../output/processed_happydb.csv")




```

```{r for practice}
txt <- cleaned_hm$cleaned_hm
txt_corpus <-VCorpus(VectorSource(txt)) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, words = character(0))


stop_dict <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))


a[!(a$lexicon %in% unique(a$lexicon)[-4]), 'lexicon'] <- 'updated'
```

































