library(tm)
library(tidytext)
library(tidyverse)
library(DT)
cleaned_hm <- read_csv(
file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/cleaned_hm.csv')
# the read_csv is faster comparing to read.csv, especially for large file
# original_hm <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/original_hm.csv')
# demographic <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/demographic.csv')
# senselabel <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/senselabel.csv')
# vad <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/vad.csv')
text <- cleaned_hm$cleaned_hm
length(grep("[A-Z]{5,}", text)) # check if there exists five consecutive capital letters
#length is nonzero, so there are moments with consecutive five capital letter (in another word, have not been lowercased)
#method one
# lowertext <- tolower(text)
#method 2, use tidytext
# need to create a corpus first
text_corpus <- VCorpus(VectorSource(text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
#length is nonzero, so there are moments with consecutive five capital letter (in another word, have not been lowercased)
#method one
# lowertext <- tolower(text)
#method 2, use tidytext
# need to create a corpus first
text_corpus <- VCorpus(VectorSource(text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
# the content_transformer simply creates a function, which is tolower in deault fun, and apply to text through tm_map
# with the content transformer, you can create specified fcn for further use
text_corpus <- tm_map(text_corpus, removePunctuation)
# the content_transformer simply creates a function, which is tolower in deault fun, and apply to text through tm_map
# with the content transformer, you can create specified fcn for further use
text_corpus <- tm_map(text_corpus, removePunctuation)
text_corpus <- tm_map(text_corpus, removeNumbers)
text_corpus <- tm_map(text_corpus, removeWords, words = character(0))
# in the tm_map fcn, tm_map(text, fcn, fcn arguments)
text_corpus <- tm_map(text_corpus, stripWhitespace)
#stem words and transfer to tidy tibble
stemmed_text <- tm_map(text_corpus, stemDocument)
#stem words and transfer to tidy tibble
stemmed_text <- tm_map(text_corpus, stemDocument)
stemmed_text <- tidy(stemmed_text)
stemmed_text <- select(stemmed_text, text)
#create dictionary and tokenizing
dict <- tidy(text_corpus)
#create dictionary and tokenizing
dict <- tidy(text_corpus)
dict <- select(dict, text)
dict <- unnest_tokens(dict, dictionary, text)
# Adding custom stopwords (i.e meaningless words that wont help in further analysis)
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past")
# the name here has to be the same as the stop_words column name, which is 'word'
# so that it can be binded successfully
stop_dict <- bind_rows(stop_words, data.frame(word))
stop_dict[!(stop_dict$lexicon %in% unique(stop_dict$lexicon)[-4]), 'lexicon'] <- 'updated'
nchar(stemmed_text)
length(stemmed_text)
length(cleaned_hm$cleaned_hm)
sum(nchar(cleaned_hm$cleaned_hm))
nchar(cleaned_hm$cleaned_hm)
?unnest_tokens
a <- unnest_tokens(cleaned_hm, tokens, cleaned_hm)
length(unique(a$tokens))
length(a$tokens)
grep('[0-9]',a$tokens)
a$tokens[22]
rm(a)
a <- cleaned_hm$cleaned_hm
a <- removeNumbers(text)
grep('[0-9]', a)
grep(' ', a)
grep('character(0)', a)
a <- stripWhitespace(a)
grep('character(0)', a)
grep(' ', a)
a[1]
grep('  ', a)
grep('  ', text)
rm(a)
a <- stripWhitespace(text)
rm(a)
length(stemmed_text)
stemmed_text
stemmed_text[1]
View(stemmed_text)
size(stemmed_text)
ncol(stemmed_text)
nrow(stemmed_text)
length(cleaned_hm)
View(dict)
?mutate
stemmed_text[1,1]
a <- mutate(stemmed_text, id = row_number())
View(a)
?mutate
?row_number
row_number(cleaned_hm)
a <- mutate(stemmed_text, id = row_number())
a <- unnest_tokens(a, stems, text)
View(a)
# combine stemmed words and dictionary together
completed_text <- mutate(stemmed, id = row_number())
# combine stemmed words and dictionary together
completed_text <- mutate(stemme_text, id = row_number())
# combine stemmed words and dictionary together
completed_text <- mutate(stemmed_text, id = row_number())
# mutate adds another variable, which is row_number, so that when you unnest sentences into words
# you will know which word in which sentence by the id variable(row_number)
completed_text <- unnest_tokens(completed_text, stems, text)
?binf_col
?bind_col
?bind_cols
a <- bind_cols(a, dict)
View(a)
completed_text <- bind_cols(completed_text, dict)
View(completed_text)
?anti_join
completed_text <- anti_join(completed_text, stop_dict, by = c('dictionary' == 'word'))
completed_text <- anti_join(completed_text, stop_dict, by = c('dictionary' = 'word'))
View(completed_text)
rm(a)
?group_by
a <- group_by(completed_text, stems)
View(a)
identical(a, completed_text)
View(completed_text)
completed_text$stems[183921]
completed_text$stems[205785]
completed_text$stems[205789]
a <- ungroup(a)
identical(a,completed_text)
?counnt
?count
a <- count(completed_text$dictionary)
a <- group_by(completed_text, stems)
b <- count(completed_text, dictionary)
View(b)
length(unique(completed_text$dictionary))
length(unique(completed_text$stems))
length(grep('abandoned', completed_text$dictionary))
count(completed_text, dictionary)
c <- count(completed_text, dictionary)
identical(b,c)
?mutate
d <- mutate(c, word = dictionary[which.max(n)])
View(d)
which.max(n)
?which.max()
d <- cleaned_hm$cleaned_hm[which.max(n)]
completed_text$stems[which.max(n)]
d <- completed_text$stems[which.max(n)]
completed_text$dictionary[which.max(n)]
d <- c$dictionary[which.max(n)]
rm(a,b,c,d)
?select
#get frequency using group_by and count
completed_text <- group_by(completed_text, stems)
completed_text <- count(completed_text, dictionary)
completed_text <- mutate(completed_text, word = dictionary[which.max(n)])
View(completed_text)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
cleaned_hm <- read_csv(
file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/cleaned_hm.csv')
# the read_csv is faster comparing to read.csv, especially for large file
# original_hm <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/original_hm.csv')
# demographic <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/demographic.csv')
# senselabel <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/senselabel.csv')
# vad <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/vad.csv')
text <- cleaned_hm$cleaned_hm
length(grep("[A-Z]{5,}", text)) # check if there exists five consecutive capital letters
#length is nonzero, so there are moments with consecutive five capital letter (in another word, have not been lowercased)
#method one
# lowertext <- tolower(text)
#method 2, use tidytext
# need to create a corpus first
text_corpus <- VCorpus(VectorSource(text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
# the content_transformer simply creates a function, which is tolower in deault fun, and apply to text through tm_map
# with the content transformer, you can create specified fcn for further use
text_corpus <- tm_map(text_corpus, removePunctuation)
text_corpus <- tm_map(text_corpus, removeNumbers)
text_corpus <- tm_map(text_corpus, removeWords, words = character(0))
# in the tm_map fcn, tm_map(text, fcn, fcn arguments)
text_corpus <- tm_map(text_corpus, stripWhitespace)
# stripWhitespace will turn multiple spaces into one space blank
#stem words and transfer to tidy tibble
stemmed_text <- tm_map(text_corpus, stemDocument)
stemmed_text <- tidy(stemmed_text)
stemmed_text <- select(stemmed_text, text)
#create dictionary and tokenizing
dict <- tidy(text_corpus)
dict <- select(dict, text)
dict <- unnest_tokens(dict, dictionary, text)
# unnest_tokens(data, output column name, input column name)
# unnest_token basically split words in sentences
# Adding custom stopwords (i.e meaningless words that wont help in further analysis)
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past")
# the name here has to be the same as the stop_words column name, which is 'word'
# so that it can be binded successfully
stop_dict <- bind_rows(stop_words, data.frame(word))
stop_dict[!(stop_dict$lexicon %in% unique(stop_dict$lexicon)[-4]), 'lexicon'] <- 'updated'
# combine stemmed words and dictionary together
completed_text <- mutate(stemmed_text, id = row_number())
# mutate adds another variable, which is row_number, so that when you unnest sentences into words
# you will know which word in which sentence by the id variable(row_number)
completed_text <- unnest_tokens(completed_text, stems, text)
completed_text <- bind_cols(completed_text, dict)
completed_text <- anti_join(completed_text, stop_dict, by = c('dictionary' = 'word'))
# the anti_join will return rows in the first table that have no matches on the second table
# by = c('dict' = 'word') works when same variable in two tabels have different names
# so it can treat these two variabels as the same one , and join by this variable
a <- completed_text
b <- count(a, dictionary)
b <- mutate(b, word = dictionary[which.max(n)])
View(b)
b <- ungroup(b)
View(b)
rmï¼ˆb)
b <- group_by(a, stems)
b <- count(b, dictionary)
b <- mutate(b, word = dictionary[which.max()])
b <- mutate(b, word = dictionary[which.max(n)])
View(b)
b <- select(b, stems, word
b <- select(b, stems, word)
View(b)
b <- select(b, stems, word)
b <- select(b, c(stems,word))
c <- distinct(b)
View(c)
>distinct
?distinct
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
completed <- stemmed %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict) %>%
anti_join(stop_words, by = c("dictionary" = "word"))
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
View(completed)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
completed <- stemmed %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict) %>%
anti_join(stop_words, by = c("dictionary" = "word"))
a <- completed
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
>right_join()
?right_join()
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
cleaned_hm <- read_csv(
file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/cleaned_hm.csv')
# the read_csv is faster comparing to read.csv, especially for large file
# original_hm <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/original_hm.csv')
# demographic <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/demographic.csv')
# senselabel <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/senselabel.csv')
# vad <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/vad.csv')
text <- cleaned_hm$cleaned_hm
length(grep("[A-Z]{5,}", text)) # check if there exists five consecutive capital letters
#length is nonzero, so there are moments with consecutive five capital letter (in another word, have not been lowercased)
#method one
# lowertext <- tolower(text)
#method 2, use tidytext
# need to create a corpus first
text_corpus <- VCorpus(VectorSource(text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
# the content_transformer simply creates a function, which is tolower in deault fun, and apply to text through tm_map
# with the content transformer, you can create specified fcn for further use
text_corpus <- tm_map(text_corpus, removePunctuation)
text_corpus <- tm_map(text_corpus, removeNumbers)
text_corpus <- tm_map(text_corpus, removeWords, words = character(0))
# in the tm_map fcn, tm_map(text, fcn, fcn arguments)
text_corpus <- tm_map(text_corpus, stripWhitespace)
# stripWhitespace will turn multiple spaces into one space blank
#stem words and transfer to tidy tibble
stemmed_text <- tm_map(text_corpus, stemDocument)
stemmed_text <- tidy(stemmed_text)
stemmed_text <- select(stemmed_text, text)
#create dictionary and tokenizing
dict <- tidy(text_corpus)
dict <- select(dict, text)
dict <- unnest_tokens(dict, dictionary, text)
# unnest_tokens(data, output column name, input column name)
# unnest_token basically split words in sentences
# Adding custom stopwords (i.e meaningless words that wont help in further analysis)
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past")
# the name here has to be the same as the stop_words column name, which is 'word'
# so that it can be binded successfully
stop_dict <- bind_rows(stop_words, data.frame(word))
stop_dict[!(stop_dict$lexicon %in% unique(stop_dict$lexicon)[-4]), 'lexicon'] <- 'updated'
# combine stemmed words and dictionary together
completed_text <- mutate(stemmed_text, id = row_number())
# mutate adds another variable, which is row_number, so that when you unnest sentences into words
# you will know which word in which sentence by the id variable(row_number)
completed_text <- unnest_tokens(completed_text, stems, text)
completed_text <- bind_cols(completed_text, dict)
completed_text <- anti_join(completed_text, stop_dict, by = c('dictionary' = 'word'))
# the anti_join will return rows in the first table that have no matches on the second table
# by = c('dict' = 'word') works when same variable in two tabels have different names
# so it can treat these two variabels as the same one , and join by this variable
copy <- completed_text
#get frequency using group_by and count
completed_text <- group_by(completed_text, stems)
completed_text <- count(completed_text, dictionary)
completed_text <- mutate(completed_text, word = dictionary[which.max(n)])
# the group_by fcn affects here, so wordd variable will show the most frequent word given they have the same stems
completed_text <- ungroup(completed_text)
completed_text <- select(completed_text, stems, word)
completed_text <- distinct(completed_text)
View(completed_text)
View(copy)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
cleaned_hm <- read_csv(
file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/cleaned_hm.csv')
# the read_csv is faster comparing to read.csv, especially for large file
# original_hm <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/original_hm.csv')
# demographic <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/demographic.csv')
# senselabel <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/senselabel.csv')
# vad <- read_csv(
#   file = 'D:/Bo Peng/Columbia/Spring 2019/Applied Data Science 5243/Project-1/data/vad.csv')
text <- cleaned_hm$cleaned_hm
length(grep("[A-Z]{5,}", text)) # check if there exists five consecutive capital letters
#length is nonzero, so there are moments with consecutive five capital letter (in another word, have not been lowercased)
#method one
# lowertext <- tolower(text)
#method 2, use tidytext
# need to create a corpus first
text_corpus <- VCorpus(VectorSource(text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
# the content_transformer simply creates a function, which is tolower in deault fun, and apply to text through tm_map
# with the content transformer, you can create specified fcn for further use
text_corpus <- tm_map(text_corpus, removePunctuation)
text_corpus <- tm_map(text_corpus, removeNumbers)
text_corpus <- tm_map(text_corpus, removeWords, words = character(0))
# in the tm_map fcn, tm_map(text, fcn, fcn arguments)
text_corpus <- tm_map(text_corpus, stripWhitespace)
# stripWhitespace will turn multiple spaces into one space blank
#stem words and transfer to tidy tibble
stemmed_text <- tm_map(text_corpus, stemDocument)
stemmed_text <- tidy(stemmed_text)
stemmed_text <- select(stemmed_text, text)
#create dictionary and tokenizing
dict <- tidy(text_corpus)
dict <- select(dict, text)
dict <- unnest_tokens(dict, dictionary, text)
# unnest_tokens(data, output column name, input column name)
# unnest_token basically split words in sentences
# Adding custom stopwords (i.e meaningless words that wont help in further analysis)
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past")
# the name here has to be the same as the stop_words column name, which is 'word'
# so that it can be binded successfully
stop_dict <- bind_rows(stop_words, data.frame(word))
stop_dict[!(stop_dict$lexicon %in% unique(stop_dict$lexicon)[-4]), 'lexicon'] <- 'updated'
# combine stemmed words and dictionary together
completed_text <- mutate(stemmed_text, id = row_number())
# mutate adds another variable, which is row_number, so that when you unnest sentences into words
# you will know which word in which sentence by the id variable(row_number)
completed_text <- unnest_tokens(completed_text, stems, text)
completed_text <- bind_cols(completed_text, dict)
completed_text <- anti_join(completed_text, stop_dict, by = c('dictionary' = 'word'))
# the anti_join will return rows in the first table that have no matches on the second table
# by = c('dict' = 'word') works when same variable in two tabels have different names
# so it can treat these two variabels as the same one , and join by this variable
#get frequency using group_by and count
copy <- completed_text
completed_text <- group_by(completed_text, stems)
completed_text <- count(completed_text, dictionary)
completed_text <- mutate(completed_text, word = dictionary[which.max(n)])
# the group_by fcn affects here, so wordd variable will show the most frequent word given they have the same stems
completed_text <- ungroup(completed_text)
completed_text <- select(completed_text, stems, word)
completed_text <- distinct(completed_text)
#here it doesnt specify which variabe to check uniqueness so use all of them
completed_text <- right_join(completed_text, copy)
completed_text <- select(completed_text, -stems)
?summarise
completed_text <- group_by(completed_text, id)
completed_text <- summarise(completed_text, text = str_c(word, collapse = ' '))
completed_text <- ungroup(completed_text)
View(completed_text)
View(cleaned_hm)
View(completed_text)
?datatable
revised_df <- cleaned_hm
revised_df <- mutate(revised_df, id = row_number())
revised_df <- inner_join(revised_df, completed_text)
datatable(revised_df)
